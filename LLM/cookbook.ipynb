{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbd5298-2c2d-4388-b165-d384d9e7109e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82358183-be9a-4472-bc60-c4a13f7d3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4c11d-159f-4096-bd78-2fefce69f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "def callBack(text):\n",
    "  messages = [\n",
    "      SystemMessage(\"You are a helpful assistant and answer the question as best as you can.\"),\n",
    "      HumanMessage(text),\n",
    "  ]\n",
    "  return model.invoke(messages).content\n",
    "  \n",
    "callBack(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f3e6b-3072-439b-bda2-4c9a49fe0136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece18b3-bdbf-473f-933b-bfe1fa6e207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
    "    )\n",
    "    language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "\n",
    "inp = \"I feel very bad for missing the train.\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "response\n",
    "\n",
    "response.model_dump() # dict output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6dbf667-d332-4a1f-b5e4-7c92747d5475",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979805b-d11c-4b90-88dc-14a26ea1cd51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34381d8d-67ff-4931-b1dc-11c600e1356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chat_models import init_chat_model\n",
    "import requests\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' times 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the 'y'.\"\"\"\n",
    "    return x**y\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str = \"New York\") -> str:\n",
    "    \"\"\"Fetch the weather report for a given 'city'.\"\"\"\n",
    "    return f\"The current temperature in {city} is 15Â°C with mild snow fall.\"\n",
    "\n",
    "@tool\n",
    "def get_traffic_report(city: str) -> str:\n",
    "    \"\"\"Fetch the current traffic report for a given 'city'.\"\"\"\n",
    "    return f\"Traffic in {city}: Moderate traffic with occasional congestion.\"\n",
    "\n",
    "# List of tools including the new weather tool\n",
    "tools = [multiply, exponentiate, add, get_weather, get_traffic_report]\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant\"), \n",
    "    (\"human\", \"{input}\"), \n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\t\t\n",
    "])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "# Create the agent with the tools and prompt\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Step 4: Invoke the tool with a query including the weather request\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"How's the traffic in New York?\",\n",
    "})\n",
    "\n",
    "# Step 5: Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d8943-1d90-4e47-becf-f8c09a0da458",
   "metadata": {},
   "source": [
    "# News Scriptwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65943547-64a9-435d-b2f0-b477cda15260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "def callBack(text):\n",
    "  messages = [\n",
    "      SystemMessage(\"You are a helpful smart news scriptwriter capable of intrepreting news from internet scrapped news article.\"),\n",
    "      HumanMessage(text),\n",
    "  ]\n",
    "  return model.invoke(messages).content\n",
    "  \n",
    "callBack(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745b40b-a959-4456-a4ba-38a833655fd8",
   "metadata": {},
   "source": [
    "# Junior News Scriptwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b8ce0-1852-44d8-b182-997e44ca2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Article(BaseModel):\n",
    "    \"\"\"You are a intelligent AI, capable of identifying the genre of a news article.\"\"\"\n",
    "    \n",
    "    title: str = Field(description=\"Title of the news article\")\n",
    "    genre: str = Field(description=\"Genre of the news article\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How important the news article is, from 1 to 10\"\n",
    "    )\n",
    "    \n",
    "llm = llm.with_structured_output(Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e98a9c-d886-4ec1-a7b1-dd5fac37686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callBack(text):\n",
    "  return llm.invoke(text)\n",
    "  \n",
    "callBack(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aefd5a-8943-477e-939b-968a42a4d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://python.langchain.com/docs/how_to/structured_output/#the-with_structured_output-method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304a2a4-4d17-4640-a86c-ea9588555dc9",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a4fec-7603-4fe4-bcd7-d53f2313796a",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18681d71-e677-491c-bdff-1f507cb07984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "# embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# vector db - create\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# vector db - write\n",
    "import os\n",
    "import re\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "directory = \"./tmpd/text/\"\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "documents = []\n",
    "for file in sorted_alphanumeric(os.listdir(\"./tmpd/text/\")):\n",
    "  full_path = os.path.join(directory, file)\n",
    "  print(full_path)\n",
    "  with open(full_path, 'r') as f:\n",
    "    documents.append(\n",
    "    \tDocument(page_content=f.read(), metadata={\"source\": \"jokes\"},)\n",
    "    \t)\n",
    "\t\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "# vector db - save\n",
    "vector_store.save_local(\"qwen_knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1af87-b2dc-405a-8215-15ab4dc7de62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d12d0e-5e5e-4544-961f-34a5c3902590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "# embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# load vector db\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.load_local(\n",
    "    \"qwen_knowledge\", embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc763aa-6f6b-4f75-831c-4a5e3461df62",
   "metadata": {},
   "source": [
    "## Ingestion and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4d337-0da6-475a-ac35-38ecbdf3d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"qwen2.5:latest\", model_provider=\"ollama\")\n",
    "\n",
    "# embedding\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04573df-52f6-4c28-87b3-f80a00b234e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81567f5b-eee3-44a9-9293-f7e82261dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/vectorstores/faiss/#saving-and-loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
